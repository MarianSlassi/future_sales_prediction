{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256dfb03",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d12160",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25182090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import gc\n",
    "from tqdm import notebook\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset\n",
    "# sales = pd.read_csv('../data/sales_train.csv')\n",
    "sales = pd.read_parquet('../data/output/sales_cleaned.parquet')\n",
    "# Data-Dicts\n",
    "items = pd.read_csv('../data/dicts/items.csv')\n",
    "items_categories = pd.read_csv('../data/dicts/item_categories.csv')\n",
    "shops = pd.read_csv('../data/dicts/shops.csv')\n",
    "\n",
    "# Folder - Sumbission data \n",
    "submission = pd.read_csv('../data/submission_data/sample_submission.csv')\n",
    "# a sample submission file in the correct format.\n",
    "\n",
    "test = pd.read_csv('../data/submission_data/test.csv') \n",
    "# the test set. You need to forecast the sales \n",
    "# for these shops and products for November 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced0524",
   "metadata": {},
   "source": [
    "## full_df - full schema of shops with dicts and target columns adding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a13d7",
   "metadata": {},
   "source": [
    "### Creating full schema of montly sold items for every shop - { df } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs_combination_by = ['date_block_num', 'shop_id', 'item_id']\n",
    "all_shops_items = []\n",
    "\n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    unique_shops = sales[sales['date_block_num'] == block_num]['shop_id'].unique()\n",
    "    unique_items = sales[sales['date_block_num'] == block_num]['item_id'].unique()\n",
    "    all_shops_items.append(np.array(list(itertools.product([block_num], unique_shops, unique_items)), dtype='int32'))\n",
    "\n",
    "df = pd.DataFrame(np.vstack(all_shops_items), columns=all_obs_combination_by, dtype='int32')\n",
    "df # full schema with all unique combinations of month number, shop_id, and item_id for month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643017d",
   "metadata": {},
   "source": [
    "### Making a target feature and outliers flags from basic dataframe - { aggregated }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = sales.groupby(all_obs_combination_by).agg({'item_price'  : 'mean', 'item_cnt_day': 'sum','was_item_price_outlier':'mean', 'was_item_cnt_day_outlier':'mean'})\n",
    "aggregated.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace = True)\n",
    "aggregated[aggregated['was_item_cnt_day_outlier']!=0] # creating additional column for sales as a target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe06ea",
   "metadata": {},
   "source": [
    "### Merging full schema with agregated by target basic dataframe - contact { full_df } + {test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(df, aggregated, on = all_obs_combination_by, how = 'left')\n",
    "full_df.fillna(value = 0, inplace= True)\n",
    "\n",
    "test['date_block_num'] = 34\n",
    "full_df = pd.concat([full_df, test], ignore_index= True)\n",
    "full_df = full_df.drop('ID',axis=1)\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aac6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(full_df[full_df['item_cnt_month']==0]).shape[0] /  full_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59905704",
   "metadata": {},
   "source": [
    "### Adding dicts on { full_df }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519697f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shops.head(5))\n",
    "print('\\n')\n",
    "print(items.head(1))\n",
    "print('\\n')\n",
    "print(items_categories.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_categories['general_item_category_name'] = items_categories['item_category_name'].\\\n",
    "    apply(lambda x: 'Игровые консоли' if x.split()[0] == 'Игровые' else x.split()[0] )\n",
    "items_categories['general_item_category_name'] = pd.Categorical(items_categories.general_item_category_name).codes\n",
    "items_categories = items_categories.drop('item_category_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76151a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "shops['city'] = shops['shop_name'].apply(lambda x: 'Якутск' if x.split()[0] == '!Якутск' else x.split()[0] )\n",
    "shops['city'] = pd.Categorical(shops.city).codes\n",
    "shops = shops.drop('shop_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.drop('item_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shops.head(5))\n",
    "print('\\n')\n",
    "print(items.head(1))\n",
    "print('\\n')\n",
    "print(items_categories.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.merge(items, on='item_id', how='left')\n",
    "full_df = full_df.merge(items_categories, on = 'item_category_id', how = 'left')\n",
    "full_df = full_df.merge(shops, on = 'shop_id', how = 'left')\n",
    "full_df # sales DataFrame with full schema of monthly items, aggregated by monthes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8cbd0",
   "metadata": {},
   "source": [
    "We will add dicts data to sales as well for future tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.merge(items, on='item_id', how='left')\n",
    "sales = sales.merge(items_categories, on = 'item_category_id', how = 'left')\n",
    "sales = sales.merge(shops, on = 'shop_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc95fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = sales.memory_usage(deep=True).sum()\n",
    "size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Memory usage full_df: {size_in_megabytes:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2617ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = full_df.memory_usage(deep=True).sum()\n",
    "size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Memory usage full_df: {size_in_megabytes:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a09363",
   "metadata": {},
   "source": [
    "## Item_cnt_day aggregation based on other features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b6a57",
   "metadata": {},
   "source": [
    "Aggregating target not only for unique combinations, but also in generall for shops, item_id, category, general_category, and city "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c889c0f",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b18c5",
   "metadata": {},
   "source": [
    "We will creater features according to our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# all_obs_combination_by\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','item_id'], as_index= False)[['item_cnt_day']].sum()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_item_id_total'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','item_id'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','shop_id'], as_index= False)[['item_cnt_day']].sum()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_shop_id_total'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','shop_id'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','item_category_id'], as_index= False)[['item_cnt_day']].sum()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_category_total'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','item_category_id'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','general_item_category_name'], as_index= False)[['item_cnt_day']].sum()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_general_category_total'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','general_item_category_name'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','city'], as_index= False)[['item_cnt_day']].sum()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_city_total'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','city'], how= 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04abeb8",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bef451",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "temp = sales.groupby(by=['date_block_num','item_id'], as_index= False)[['item_cnt_day']].mean()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_item_id_mean'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','item_id'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','shop_id'], as_index= False)[['item_cnt_day']].mean()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_shop_id_mean'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','shop_id'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','item_category_id'], as_index= False)[['item_cnt_day']].mean()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_category_mean'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','item_category_id'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','general_item_category_name'], as_index= False)[['item_cnt_day']].mean()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_general_category_mean'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','general_item_category_name'], how= 'left')\n",
    "\n",
    "temp = sales.groupby(by=['date_block_num','city'], as_index= False)[['item_cnt_day']].mean()\n",
    "temp = temp.rename(columns={'item_cnt_day': 'target_by_city_mean'})\n",
    "full_df = pd.merge(full_df, temp, on =['date_block_num','city'], how= 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6388df8",
   "metadata": {},
   "source": [
    "These statistics helps us to have a full view of target change across different coordinates. Let's take one observation as example. So, we have an item sold in some amount in the current month at some shop, but now we also know, how many of this item we sold in other shops in this month, and how many items of this category we sold, as well as how many those items we sold in this current city. So, now we have more data, even if we look at one raw observation - all data already gathered and reflected in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e0c15",
   "metadata": {},
   "source": [
    "We still have our data marked as data_block_num == 34, what allowed us not to intercept since we've been grouping this by using unique date_block_nums from original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fe524",
   "metadata": {},
   "source": [
    "Let's also add \"soft\" aggragated data, which includes target data only from past monthes for every \"current\" month, with the same logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad262364",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989906b",
   "metadata": {},
   "source": [
    "## First month sold items - { first_month_item_id } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['not_full_historical_data'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd2700",
   "metadata": {},
   "source": [
    "We would like to see how many items were sold first time in which monthes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4800c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_month = full_df.groupby('item_id', as_index=False)['date_block_num'].min()\n",
    "first_month['date_block_num'].value_counts().sort_index() # checking - no transformations in this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcf9f9",
   "metadata": {},
   "source": [
    "As we can see more items started to be sold in the fisrt month, and it's better to assign to these observations \"not_full_historical_data\" flag positive value. Cause information which we want to extract with this feature - is items and prices outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_month = full_df.groupby('item_id', as_index=False)['date_block_num'].min()\n",
    "first_month.rename(columns={'date_block_num': 'first_month_item_id_num'}, inplace=True)\n",
    "\n",
    "full_df = full_df.merge(first_month, on='item_id', how='left')\n",
    "\n",
    "full_df['first_month_item_id'] = (full_df['date_block_num'] == full_df['first_month_item_id_num']).astype('int8')\n",
    "full_df = full_df.drop('first_month_item_id_num', axis = 1)\n",
    "full_df.loc[full_df['date_block_num'] == 0, 'not_full_historical_data'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af49815",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[(full_df['first_month_item_id'] == 1)&(full_df['was_item_cnt_day_outlier'] == 1) ]\\\n",
    "    ['date_block_num'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c6a44",
   "metadata": {},
   "source": [
    "We found clear split, where some items were sold in first month and were \"target\" (item_cnt_day) outliers as well. Despite the fact 'was_item_price_outlier' in full_df DataFrame is aggregated feature with value only rarelly equal to 1.0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b590e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df[full_df['date_block_num'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93245475",
   "metadata": {},
   "source": [
    "## Expanding window target aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c1df8",
   "metadata": {},
   "source": [
    "['shop_id','item_id], ['shop_id], ['item_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49297b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregating_target_by = [['item_id', 'shop_id'], ['item_id'], ['shop_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for feature in aggregating_target_by:\n",
    "    col = '_'.join(['target_aggregated_mean_premonthes', *feature])\n",
    "    col2 = '_'.join(['target_aggregated_max_premonthes', *feature])\n",
    "    full_df[col] = np.nan\n",
    "    full_df[col2] = np.nan\n",
    "\n",
    "    for d in notebook.tqdm(full_df.date_block_num.unique()):\n",
    "        valid_month = (full_df.date_block_num < d)\n",
    "        current_month = (full_df.date_block_num == d)\n",
    "\n",
    "        temp = full_df.loc[valid_month].groupby(feature)[['item_cnt_month']].mean().reset_index()\n",
    "        agg = full_df.loc[current_month][feature].merge(temp, on=feature, how='left')[['item_cnt_month']].copy()\n",
    "        agg.set_index(full_df.loc[current_month].index, inplace=True)\n",
    "        full_df.loc[current_month, col] = agg['item_cnt_month']\n",
    "\n",
    "        temp = full_df.loc[valid_month].groupby(feature)[['item_cnt_month']].max().reset_index()\n",
    "        agg = full_df.loc[current_month][feature].merge(temp, on=feature, how='left')[['item_cnt_month']].copy()\n",
    "        agg.set_index(full_df.loc[current_month].index, inplace=True)\n",
    "        full_df.loc[current_month, col2] = agg['item_cnt_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b5b63",
   "metadata": {},
   "source": [
    "Since this operation takes 6 minutes to load, i will make it easier to debug future steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_parquet('full_df.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286396b",
   "metadata": {},
   "source": [
    "## Data Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_parquet('full_df.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fa509",
   "metadata": {},
   "source": [
    "### Don't we have some NaNs after target aggreagtion from previous monthes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[full_df['date_block_num'] == 1].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798012b",
   "metadata": {},
   "source": [
    "Any missed target_aggregated_mean_premonthes_item_id_shop_id from previous monthes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32986099",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[(full_df['target_aggregated_mean_premonthes_item_id_shop_id'].isnull())\\\n",
    "         & (full_df['date_block_num']>0)]\\\n",
    "            [['date_block_num']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e22172",
   "metadata": {},
   "source": [
    "Let's see without monthes when item was sold first time --> has no data to be aggregated from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[(full_df['target_aggregated_mean_premonthes_item_id_shop_id'].isnull())\\\n",
    "         & (full_df['date_block_num']>0) & (full_df['first_month_item_id']!= 1 )]\\\n",
    "            [['date_block_num']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case we can add flag for lack of info for target_aggregated_mean_premonthes_item_id_shop_id with the following code \n",
    "# full_df[(full_df['target_aggregated_mean_premonthes_item_id_shop_id'].isnull())\\\n",
    "#          & (full_df['date_block_num']>0) & (full_df['first_month_item_id']!= 1 )]['not_full_historical_data'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f2922",
   "metadata": {},
   "source": [
    "And now let's see if there any shops which have no historical aggregated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case we can add flag for lack of info for shops which were selling first month with the following code \n",
    "\n",
    "full_df[(full_df['target_aggregated_mean_premonthes_shop_id'].isnull())\\\n",
    "         & (full_df['date_block_num']>0) & (full_df['first_month_item_id']!= 1 )][['date_block_num']].value_counts().sort_index() # To see what's those NaNs are\n",
    "\n",
    "# full_df[(full_df['target_aggregated_mean_premonthes_shop_id'].isnull())\\\n",
    "#          & (full_df['date_block_num']>0) & (full_df['first_month_item_id']!= 1 )]['not_full_historical_data'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e83d4",
   "metadata": {},
   "source": [
    "After our target aggregation we can see, that we have NaN values, somewehere, because it's first month when item sold, and there no historical data for that item, not possible to collect info about it from previous monthes. We doing well, bacause those observations still have flag for first month selling, they are marked for the model. But what are remaining NaNs, why they still exist if we filter by first month sold items? Those are when item sold not a first time in this month, but sold first time in this shop. Let's check it below only for item_id target value (was it sold prevoisly and has historical data, and was it first time when it sold?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040446d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[(full_df['target_aggregated_mean_premonthes_item_id'].isnull())\\\n",
    "         & (full_df['date_block_num']>0)]\\\n",
    "            [['date_block_num']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[(full_df['target_aggregated_mean_premonthes_item_id'].isnull())\\\n",
    "         & (full_df['date_block_num']>0) & (full_df['first_month_item_id']!= 1 )]\\\n",
    "            [['date_block_num']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6621c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[full_df['date_block_num']==34].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317bd9af",
   "metadata": {},
   "source": [
    "### What else? - follow up on the plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb9562",
   "metadata": {},
   "source": [
    "**Basic features:**\n",
    "\n",
    "- ✅City of the shop  \n",
    "- ✅Category of the product (item)  \n",
    "- ✅General product category (item)  \n",
    "- ✅Monthly sales of exact category in shop `item_cnt_month` (it will be our target) \n",
    "\n",
    "**We also should add some lag features based on statistical metrics for exact shops, item_id's, and categories:**\n",
    "\n",
    "- ✅Total / Mean amount of sold exact `item_id` in this month (How many of this `item_id` we sold in this month?)  \n",
    "- ✅Total / Mean amount of `item_cnt_day` sold this month in the exact shop (How many items this shop sold this month?)  \n",
    "- ✅Total / Mean amount of product category sold in this month (How well this category sold during this month?)  \n",
    "- ✅Total / Mean amount of general product category sold in this month (How well \"Movies\" category sold during this month?)\n",
    "\n",
    "**Combined:**\n",
    "- ❌Mean of how many items of exact category exact shop sell per month (How good this shop sells products of this cateogry?)\n",
    "\n",
    "\n",
    "**Next:**\n",
    "- Then we need to add lags for 1, 2, 3, 12 month periods for all features mentioned above  \n",
    "- ✅Also, would be nice to add a feature like `not_full_historical_data` for the first three months and for monthes in the first year\n",
    "- Add \"deltas\" for target - as how amount of sold items been changed for the last monthes of this `shop_id` and `item_id` \n",
    "- ✅Add binary feature `first_month_item_id` which reflects if exact item will be sold first time in this month\n",
    "- ✅❌Add mean sliding window for target for the last three monthes `mean_3`\n",
    "- Number of month as `month = date_block_num mod 12`\n",
    "- Add binary feature `shop_was_in_test` and `item_id_was_in_test` for shops and items which will be in test when model will predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2955a345",
   "metadata": {},
   "source": [
    "Remaining:\n",
    "- Number of month as `month = date_block_num mod 12`\n",
    "- Add binary feature `shop_was_in_test` and `item_id_was_in_test` for shops and items which will be in test when model will predict\n",
    "- Then we need to add lags for 1, 2, 3, 12 month periods for all features mentioned above  \n",
    "- Add \"deltas\" for target - as how amount of sold items been changed for the last monthes of this `shop_id` and `item_id` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135469b",
   "metadata": {},
   "source": [
    "## Year and Month Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['month'] = full_df['date_block_num'] % 12 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['year'] = 2013 + (full_df['date_block_num'] // 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f55491",
   "metadata": {},
   "source": [
    "## Shop_was_in_test and item_id_was_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_test = test['shop_id'].unique()\n",
    "item_id_test = test['item_id'].unique()\n",
    "print('Nunique shop_id in test: ', test['shop_id'].nunique())\n",
    "print('Nunique item_id in test: ',test['item_id'].nunique())\n",
    "print('\\n')\n",
    "print(\"Unique values of shop_id : \",np.sort(shop_id_test))\n",
    "print(\"Unique values of item_id\",np.sort(item_id_test))\n",
    "print('\\n')\n",
    "full_df['item_id_was_in_test'] = 0\n",
    "full_df['shop_id_was_in_test'] = 0\n",
    "full_df.loc[(full_df['item_id'].isin(item_id_test))&(full_df['date_block_num'] != 34), 'item_id_was_in_test'] =  1\n",
    "full_df.loc[full_df['shop_id'].isin(shop_id_test)&(full_df['date_block_num'] != 34), 'shop_id_was_in_test'] =  1\n",
    "\n",
    "print('Unique values of shop_id with assigned positive shop_id_was_in_test flag',np.sort(full_df[full_df['shop_id_was_in_test']== 1]['shop_id'].unique())) # Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96663324",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = full_df.memory_usage(deep=True).sum()\n",
    "size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Memory usage full_df: {size_in_megabytes:.2f} MB\")\n",
    "print(f'Amount of rows in this table: {full_df.shape[0]}')\n",
    "print(f'Amount of columns in this table: {full_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't we have any data leakege ? \n",
    "full_df.loc[(full_df['date_block_num']== 34)&((full_df['shop_id_was_in_test']!= 0 )|(full_df['item_id_was_in_test']!= 0 )), ['shop_id_was_in_test', 'item_id_was_in_test']].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40747bc3",
   "metadata": {},
   "source": [
    "## Downcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2121fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = full_df.select_dtypes(include='float64').columns\n",
    "int_cols = full_df.select_dtypes(include='int64').columns\n",
    "\n",
    "for col in float_cols:\n",
    "    max_diff = (full_df[col] - full_df[col].astype('float32')).abs().max()\n",
    "    print(f\"{col}: max precision loss when downcasted to float32 = {max_diff}\")\n",
    "\n",
    "for col in int_cols:\n",
    "    min_val = full_df[col].min()\n",
    "    max_val = full_df[col].max()\n",
    "    if min_val < -2_147_483_648 or max_val > 2_147_483_647:\n",
    "        print(f\"{col}: OVERFLOW when downcasted to int32 (values out of range)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype('float32')\n",
    "    df[int_cols] = df[int_cols].astype('int32')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60060edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = full_df.memory_usage(deep=True).sum()\n",
    "size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Memory usage full_df: {size_in_megabytes:.2f} MB\")\n",
    "print(f'Amount of rows in this table: {full_df.shape[0]}')\n",
    "print(f'Amount of columns in this table: {full_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd96294",
   "metadata": {},
   "source": [
    "item_cnt_month aggregation by shop, item_id etc. are expectably NaNs for test, for other features we can fill with 0 valu, since amount of sold items in previous monthes is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064804c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "full_df.fillna(0, inplace=True)\n",
    "full_df = downcast_dtypes(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = full_df.memory_usage(deep=True).sum()\n",
    "size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Memory usage full_df: {size_in_megabytes:.2f} MB\")\n",
    "print(f'Amount of rows in this table: {full_df.shape[0]}')\n",
    "print(f'Amount of columns in this table: {full_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740e7c7",
   "metadata": {},
   "source": [
    "## Lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73473d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs_combination_by = ['date_block_num', 'shop_id', 'item_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_columns = [c for c in full_df if 'target' in c]\n",
    "shifted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e1254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9122906",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_obs_combination_by = ['date_block_num', 'shop_id', 'item_id']\n",
    "\n",
    "shift_range = [1, 2, 3, 12]\n",
    "\n",
    "shifted_columns = [c for c in full_df if 'target' in c]\n",
    "\n",
    "for shift in shift_range:\n",
    "    temp = full_df[all_obs_combination_by + shifted_columns].copy()\n",
    "    temp['date_block_num'] = temp['date_block_num'] + shift\n",
    "\n",
    "    foo = lambda x: f'{x}_lag_{shift}' if x in shifted_columns else x\n",
    "    temp = temp.rename(columns=foo)\n",
    "\n",
    "    full_df = pd.merge(full_df, temp, on = all_obs_combination_by, how= 'left').fillna(0)\n",
    "    full_df = downcast_dtypes(full_df)\n",
    "\n",
    "    del temp\n",
    "    gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d495a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(full_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bytes = full_df.memory_usage(deep=True).sum()\n",
    "size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Memory usage full_df: {size_in_megabytes:.2f} MB\")\n",
    "print(f'Amount of rows in this table: {full_df.shape[0]}')\n",
    "print(f'Amount of columns in this table: {full_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e720b",
   "metadata": {},
   "source": [
    "### Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c3c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
