Metadata-Version: 2.4
Name: fsp_ms
Version: 0.1.1
Summary: Project for future sales prediction, data preprocessing, and model training
License-Expression: MIT
Project-URL: Homepage, https://github.com/MarianSlassi/future_sales_prediction
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENCE
Requires-Dist: scikit-learn<1.8,>=1.2
Requires-Dist: pandas<2.3,>=1.5
Requires-Dist: numpy<2.3,>=1.21
Requires-Dist: pandera>=0.10.0
Requires-Dist: tqdm==4.67.1
Requires-Dist: joblib==1.5.1
Requires-Dist: xgboost>=1.6.2
Requires-Dist: pyarrow<=20.0.0
Requires-Dist: typing-inspect>=0.6.0
Provides-Extra: dev
Requires-Dist: matplotlib==3.10.3; extra == "dev"
Requires-Dist: optuna==4.4.0; extra == "dev"
Requires-Dist: plotly==6.2.0; extra == "dev"
Requires-Dist: scipy==1.16.0; extra == "dev"
Requires-Dist: seaborn==0.13.2; extra == "dev"
Requires-Dist: mlflow==3.1.1; extra == "dev"
Requires-Dist: shap==0.48.0; extra == "dev"
Requires-Dist: ipykernel>=6.30.0; extra == "dev"
Dynamic: license-file

This is a simple librarary to transofrm data, train model, and make predictions.
If you using collab or kaggle notebook, please import this package as 
---
!pip install \
  --index-url https://test.pypi.org/simple/ \
  --extra-index-url https://pypi.org/simple \
  fse-rimka-lasso
</br></br>*Or you can use with activated .venv:*
</br>uv pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple fse-rimka-lasso


# Google Collab
To run project in google collab save raw data to /content/data as following:
```
    ğŸ¢How to save raw data: 
    content/
        â””â”€â”€data/
            â””â”€â”€01_raw/
                â”œâ”€â”€ dicts/
                â”‚   â”œâ”€â”€ item_categories.csv
                â”‚   â”œâ”€â”€ items.csv
                â”‚   â””â”€â”€ shops.csv
                â”œâ”€â”€ submission_data/
                â”‚   â”œâ”€â”€ sample_submission.csv
                â”‚   â””â”€â”€ test.csv
                â””â”€â”€ sales_train.csv
```
And run folling code
```
import gc

from fse_rimka_lasso import Config, get_logger, Validator, ETL_pipeline, SchemaSales, BuildFeatures, SchemaFeatures , Split , XGB_model
from pathlib import Path
colab_base_dir = Path("/content/data")

# Common config for all objects
config = Config(base_dir=colab_base_dir)

logger_etl = get_logger(config=config, name = "etl", log_file = config.get('log_file_etl'))
etl = ETL_pipeline(config, logger_etl)
## Init: Validator and it's logger
logger_etl_schema = get_logger(config=config, name ="validation_schema_cleaned"\
                              , log_file=config.get('validation_schema_cleaned'))
etl_validator = Validator(logger_etl_schema)
### Init: Schema for Validator
etl_schema = SchemaSales()
## Run: etl transformation with transferring validator and validation schema
etl.run(validator_object = etl_validator, validation_schema = etl_schema, dry_run= False)

del logger_etl, etl, logger_etl_schema, etl_validator, etl_schema
gc.collect()
```
### New cell
```
# FE
logger_fe = get_logger(config=config, name = "build_features", \
                    log_file = config.get('log_file_build_features'))
build_features = BuildFeatures(config, logger_fe)
## Init: Validator and it's logger
features_validation_logger = get_logger(config=config, name = "validation_schema_features", \
                    log_file = config.get('validation_schema_features'))
fe_validator = Validator(features_validation_logger)
### Init: Schema for Validator
features_schema = SchemaFeatures()

build_features.run(validator_object = fe_validator, validation_schema = features_schema, dry_run = False )

del build_features
gc.collect()

# Split

logger_split = get_logger(config=config, name = "split", \
                        log_file = config.get('log_file_split'))
split = Split(config, logger_split)
split.run()

# Model 

logger_model = get_logger(config=config, name = "model", log_file= config.get('log_file_model'))

model = XGB_model(config, logger_model)
model.train(save = True)

```
