ğŸ”­ Project structure:

    project/
    â”‚
    â”œâ”€â”€ config.py
    â”‚
    â”œâ”€â”€ reqirements.txt
    â”‚
    â”œâ”€â”€ README.MD
    â”‚
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ raw/            â† dicts + submission_data + sales_train.csv (âš ï¸check below for expected order)
    â”‚   â”œâ”€â”€ interim/        â† techincal folder, redundant but remained in case of need (âš ï¸ check below for data explanation)
    â”‚   â”œâ”€â”€ cleaned/        â† cleaned.parquet
    â”‚   â”œâ”€â”€ features/       â† final_df
    â”‚   â”œâ”€â”€ processed/      â† train/val/test
    â”‚   â”œâ”€â”€ predictions/    â† output.csv
    â”‚   â””â”€â”€ logs/
    â”‚
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ model.pkl
    â”‚
    â”œâ”€â”€ notebooks/
    â”‚   â””â”€â”€ all_notebooks.ipynb
    â”‚
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ data/
    â”‚   â”‚   â”œâ”€â”€ etl.py
    â”‚   â”‚   â””â”€â”€ split.py
    â”‚   â”œâ”€â”€ features/
    â”‚   â”‚   â””â”€â”€ build_features.py
    â”‚   â”œâ”€â”€ validation/
    â”‚   â”‚   â”œâ”€â”€ schema_cleaned.py     â† for data after cleaning
    â”‚   â”‚   â”œâ”€â”€ schema_features.py    â† for data after features engineering
    â”‚   â”‚   â””â”€â”€ validate.py           â† to chose which schema to use
    â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â”œâ”€â”€ train_model.py
    â”‚   â”‚   â””â”€â”€ predict_model.py
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ helpers.py
    â”‚       â””â”€â”€ logger.py   


    ğŸ¢How to save raw data: 
        raw/
        â”œâ”€â”€ dicts/
        â”‚   â”œâ”€â”€ item_categories.csv
        â”‚   â”œâ”€â”€ items.csv
        â”‚   â””â”€â”€ shops.csv
        â”œâ”€â”€ submission_data/
        â”‚   â”œâ”€â”€ sample_submission.csv
        â”‚   â””â”€â”€ test.csv
        â””â”€â”€ sales_train.csv


âš’ï¸ **Base workflow (run files in the following order):**
1. `project/data/raw`      âš ï¸ before starting save competition data in this directory 
2. `project/src/data/clean.py`          (schema runs automatically)
3. `project/src/features/build_features.py`     (schema runs automatically)
4. `project/src/data/split.py`      
5. `project/src/models/train_model.py`       (optionally)
6. `project/src/models/predict_model.py`      


ğŸ—ï¸ Architecture imporvements:
    -add local interpretator - uv, poetry

ğŸ“‚ Directories/paths managment by config in root:

    Notebooks:
        from pathlib import Path
        import sys
        ROOT = Path().resolve().parent
        sys.path.append(str(ROOT))
        from config import Config
        config = Config()

    Scripts:
        from pathlib import Path
        import sys
        ROOT = Path(__file__).resolve().parents[2]
        sys.path.append(str(ROOT))
        from config import Config
        config = Config()

ğŸ˜‡ Utils usage:

    ğŸ–‡ï¸Logger: 
    from src.utils.logger import get_logger
    logger = get_logger("etl", config.get('log_file_name')) # You can skip second argument. All log files managed through Config

    logger.info("Starting etl Process")
    logger.warning("Something strange happened")
    logger.error("Something went wrong")

### ğŸ“ Data: Interim

ğŸªµ **Interim** â€” contains data exported to support transformations in notebooks.

- `data_checkpoint_full_df.parquet`  
  â¤· At the feature engineering notebook, some transformations are time-consuming. This file stores an intermediate result to speed up debugging â€” useful when rerunning cells from the beginning.

- `full_df_final.csv`  
  â¤· A snapshot of the final observations after transformations in the feature engineering notebooks. Used primarily to develop validation pipelines.

- `sales_for_eda.parquet`  
  â¤· Created at the end of the DQC notebook. Unlike the cleaned version, it includes values from dicts for visualization and data consistency checks.  
  â¤· Later, the same transformation logic was moved into the EDA notebook, but a commented-out line in DQC remains for reference:
  `sales.to_parquet(config.get('sales_for_eda'), engine='pyarrow')`</br>
  â¤· You can uncomment line above in DQC, as well as import line in EDA `sales = pd.read_parquet(config.get('sales_for_eda'))` and trasform data with notebook. In this case comment out "Merging Dicts" block in EDA.










